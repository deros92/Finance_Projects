{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4729c9d",
   "metadata": {},
   "source": [
    "# Download 5-minutes data from yahoofinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's date\n",
    "today = date.today()\n",
    "# dd/mm/YY\n",
    "d1 = today.strftime(\"%m_%d_%Y\")\n",
    "\n",
    "# 40 FTSEMIB companies\n",
    "companies_dict={\n",
    "                \"A2A.MI\":\"A2A\",\"AMP.MI\":\"AMPLIFON\",\"ATL.MI\":\"ATLANTIA\",\n",
    "                \"AZM.MI\":\"AZIMUT\",\"BGN.MI\":\"BANCA GENERALI\", \"BMED.MI\":\"BANCA MEDIOLANUM\",\n",
    "                \"BAMI.MI\":\"BANCO BPM\",\"BPE.MI\":\"BPER BANCA\",\n",
    "                \"BZU.MI\":\"BUZZI UNICEM\",\"CPR.MI\":\"CAMPARI\",\"CNHI.MI\":\"CNH INDUSTRIAL\",\"DIA.MI\":\"DIASORIN\",\n",
    "                \"ENEL.MI\":\"ENEL\",\"ENI.MI\":\"ENI\",\"EXO.MI\":\"EXOR\",\n",
    "                \"RACE.MI\":\"FERRARI\",\"FBK.MI\":\"FINECO\",\"G.MI\":\"GENERALI ASSICURAZIONI\",\n",
    "                \"HER.MI\":\"HERA\", \"IP.MI\":\"INTERPUMP GROUP\",\n",
    "                \"ISP.MI\":\"INTESA SAN PAOLO\", \"INW.MI\":\"INWIT\", \"IG.MI\":\"ITALGAS\",\n",
    "                \"LDO.MI\":\"LEONARDO\",\"MB.MI\":\"MEDIOBANCA\",\"MONC.MI\":\"MONCLER\",\"NEXI.MI\":\"NEXI\", \n",
    "                \"PIRC.MI\":\"PIRELLI\",\"PST.MI\":\"POSTE ITALIANE\",\n",
    "                \"PRY.MI\":\"PRYSMIAN\",\"REC.MI\":\"RECORDATI\",\"SPM.MI\":\"SAIPEM\",\"SRG.MI\":\"SNAM\",\n",
    "                \"STLA.MI\":\"STELLANTIS\", \"STM.MI\":\"STM\",\"TIT.MI\":\"TELECOM\",\n",
    "                \"TEN.MI\":\"TENARIS\",\"TRN.MI\":\"TERNA\", \"UCG.MI\":\"UNICREDIT\",\"UNI.MI\":\"UNIPOL\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f87635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need to have every 5-minutes data ...so I create a list \n",
    "\n",
    "time_ = '09:00'\n",
    "timeList = '00:05'\n",
    "count = 0\n",
    "hours_list = []\n",
    "mysum = 0\n",
    "while str(mysum) != '17:30:00':\n",
    "    if count<1:\n",
    "        td = timedelta(hours=int(time_.split(':')[0]), minutes=int(time_.split(':')[1]))\n",
    "        d = timedelta(hours=int(timeList.split(':')[0]), minutes=int(timeList.split(':')[1]))\n",
    "        mysum = td + d\n",
    "    else:\n",
    "        d = timedelta(hours=int(timeList.split(':')[0]), minutes=int(timeList.split(':')[1]))\n",
    "        mysum = td + d\n",
    "    td = mysum\n",
    "    count+=1\n",
    "    hours_list.append('{}:{}'.format(str(mysum).split(':')[0], str(mysum).split(':')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_prices(ticker, name, folder):\n",
    "\n",
    "    df = yf.download(tickers=ticker, period='60d', interval=\"5m\")\n",
    "    # we build 'TIME' and 'DATE' variables\n",
    "    df['TIME'] =[n for n in df.reset_index()['Datetime'].dt.time]\n",
    "    df['DATE'] = [str(n).split(' ')[0] for n in df.reset_index()['Datetime']]\n",
    "    \n",
    "    # empty list; we set the date in a specific form\n",
    "    lista = []\n",
    "    for h in df['DATE']:\n",
    "        l = h.split('-')\n",
    "        l2 = [l[2],l[1],l[0]]\n",
    "        lista.append('/'.join(l2))\n",
    "    \n",
    "    df['DATE'] = lista\n",
    "    # convert DATE into datetime\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%d/%m/%Y')\n",
    "    # we set the 'TIME' into 'hours:minutes'\n",
    "    lis = []\n",
    "    for j in df['TIME']:\n",
    "        j = str(j)\n",
    "        j = j.split(':')\n",
    "        mm = [j[0], j[1]]\n",
    "        m = ':'.join(mm)\n",
    "        lis.append(m)\n",
    "    df['TIME'] = lis\n",
    "    \n",
    "    #we have to build a dataframe\n",
    "    dataframes = []\n",
    "    d_orari = pd.DataFrame({'TIME' : hours_list})\n",
    "    for h in df['DATE'].unique():\n",
    "        d_2 = df[df['DATE'] == h]\n",
    "        # yahoo finance data starts at 9:00 so we need to move 5 minutes ahead\n",
    "        d_2['TIME'] = pd.to_datetime(d_2['TIME'], format='%H:%M')\n",
    "        d_2['TIME'] = d_2['TIME'] + timedelta(0,300)\n",
    "    \n",
    "    \n",
    "        lis = []\n",
    "        for j in d_2['TIME']:\n",
    "            j = str(j)\n",
    "            j = j.split(' ')\n",
    "            mm = j[1]\n",
    "            j = mm.split(':')\n",
    "            mmm = [j[0], j[1]]\n",
    "            m = ':'.join(mmm)\n",
    "            lis.append(m)\n",
    "        \n",
    "        d_2['TIME'] = lis\n",
    "        \n",
    "        #merge with 'only hours' dataset (we want every work hours to understand if there are 5-minutes without buy/sell)\n",
    "        d_2 = d_orari.merge(d_2, on='TIME', how='left')\n",
    "        d_2['DATE'] = h\n",
    "        #d_2 = d_2.fillna(method='ffill')\n",
    "        #d['TIME'] = pd.to_datetime(d['TIME'], format='%H:%M').dt.time\n",
    "        d_2['DATE_TIME'] = d_2['DATE'].astype(str) + ' ' + d_2['TIME'].astype(str)\n",
    "        d_2 = d_2.sort_values('DATE_TIME')\n",
    "        d_2['day_of_week'] = d_2['DATE'].dt.day_name()\n",
    "        d_2['num_day_of_of_week'] = d_2['DATE'].dt.dayofweek\n",
    "        d_2['weeks'] = d_2['DATE'].dt.week\n",
    "        d_2['year'] = d_2['DATE'].dt.year\n",
    "        d_2['month'] = d_2['DATE'].dt.month\n",
    "        dataframes.append(d_2)\n",
    "    reading_concatenation = pd.concat(dataframes, ignore_index=True)\n",
    "    reading_concatenation.index = reading_concatenation['DATE_TIME']\n",
    "    reading_concatenation = reading_concatenation.rename(columns={'Open': 'OPEN', 'High':'HIGH', 'Low':'LOW','Close' :'CLOSE', 'Adj Close': 'ADJ CLOSE', 'Volume': 'VOLUME'})\n",
    "    new_df = pd.DataFrame(reading_concatenation, columns=['TIME', 'DATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE','ADJ CLOSE', 'VOLUME', 'DATE_TIME', 'day_of_week', 'num_day_of_of_week', 'weeks', 'year', 'month'])\n",
    "\n",
    "    dirName = folder + '\\{}'.format(name)\n",
    "    print(dirName)\n",
    "    if not os.path.exists(dirName):\n",
    "        os.makedirs(dirName)\n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , dirName ,  \" already exists\")\n",
    "    os.chdir(r'{}'.format(dirName))\n",
    "    new_df.to_csv('1{}_until_{}.csv'.format(name, d1), index = False)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds a file csv with all data in ourfolder\n",
    "\n",
    "def read_concatenation(name, folder):\n",
    "    dataframes = []\n",
    "    \n",
    "    dirName = folder + '\\{}'.format(name)\n",
    "    if not os.path.exists(dirName):\n",
    "        print(\"ERROR \" , dirName ,  \" doesn't exist!!!!!\")\n",
    "    else:    \n",
    "        print(\"Directory \" , dirName ,  \" already exists\")\n",
    "    directory = r'{}'.format(dirName)\n",
    "    length = len(os.listdir(directory))\n",
    "    if length < 2:\n",
    "        for filename in os.listdir(directory):\n",
    "                if filename.startswith(\"1{}\".format(name)) and filename.endswith(\".csv\"):\n",
    "                    d = pd.read_csv(directory+'/'+filename)\n",
    "                    dataframes.append(d)\n",
    "                else:\n",
    "                    print('Skip: {}'.format(filename))\n",
    "                          \n",
    "        reading_concatenation = pd.concat(dataframes, ignore_index=True)\n",
    "        cleaned_dataset = reading_concatenation.drop_duplicates(subset =\"DATE_TIME\", keep = 'first')\n",
    "        os.chdir(r'{}'.format(dirName))\n",
    "        cleaned_dataset.to_csv('Complete_{}.csv'.format(name), index = False)\n",
    "    else:\n",
    "        print('PiÃ¹ elementi in: ', name)\n",
    "        elements = os.listdir(r'{}'.format(directory))[-2:length]\n",
    "        elements.reverse()\n",
    "        for filename in elements:\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    d = pd.read_csv(directory+'/'+filename)\n",
    "                    dataframes.append(d)\n",
    "                else:\n",
    "                    print('Skip: {}'.format(filename))\n",
    "                          \n",
    "        reading_concatenation = pd.concat(dataframes, ignore_index=True)\n",
    "        cleaned_dataset = reading_concatenation.drop_duplicates(subset =\"DATE_TIME\", keep = 'last')\n",
    "        os.chdir(r'{}'.format(directory))\n",
    "        cleaned_dataset.to_csv('Complete_{}.csv'.format(name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f59d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dir = input('Select your directory: ')\n",
    "for key, value in companies_dict.items():\n",
    "    download_stock_prices(key, value, input_dir)\n",
    "for name_company in companies_dict.values():\n",
    "    read_concatenation(name_company, input_dir)\n",
    "    print('{}: Done'.format(name_company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50a8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b94b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889e7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python:finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
